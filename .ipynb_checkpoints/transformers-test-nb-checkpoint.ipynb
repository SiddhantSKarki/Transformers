{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae73c421-7e94-4771-85b9-4d10c3402b3c",
   "metadata": {},
   "source": [
    "### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c36d91a-9b74-4eb4-aa01-1d5fe192792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-12 22:48:33--  https://www.gutenberg.org/files/11/11-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 2610:28:3090:3000:0:bad:cafe:47, 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154638 (151K) [text/plain]\n",
      "Saving to: ‘11-0.txt.1’\n",
      "\n",
      "11-0.txt.1          100%[===================>] 151.01K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-10-12 22:48:33 (1.46 MB/s) - ‘11-0.txt.1’ saved [154638/154638]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.gutenberg.org/files/11/11-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d8200-e35c-4cd9-8a62-4282f0265783",
   "metadata": {},
   "source": [
    "Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499e40db-8cca-455f-a355-e37343204ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"./11-0.txt\", 'r', encoding='UTF-8').read()[664:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3bdad9-6dfc-47bd-989b-e2f250c1527b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER I.\n",
      "Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into\n",
      "the book her sister was reading, but it had no pictures or\n",
      "conversations in it, “and what is the use of a book,” thought Alice\n",
      "“without pictures or conversations?”\n",
      "\n",
      "So she was considering in her own mind (as well as she could, for the\n",
      "hot day made her feel very sleepy and stupid), whether the pleasure of\n",
      "making a daisy-chain would be worth the trouble of getting up and\n",
      "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
      "close by her.\n",
      "\n",
      "There was nothing so _very_ remarkable in that; nor did Alice think it\n",
      "so _very_ much out of the way to hear the Rabbit say to itself, “Oh\n",
      "dear! Oh dear! I shall be late!” (when she thought it over afterwards,\n",
      "it occurred to her that she ought to have wondered at this, but at the\n",
      "time it all seemed quite natural); but when the Rabbit actually _took a\n",
      "watch out of its w\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd46416a-5937-468d-b03b-b4afdf9c188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 74\n"
     ]
    }
   ],
   "source": [
    "chars = \"\". join(sorted(list(set(text))))\n",
    "print(f\"Vocab Size: {len(chars)}\")\n",
    "\n",
    "# Encoding and Decoding Mapping\n",
    "stoi = { ch: i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "\n",
    "# Encoding and Decoding Lambdas\n",
    "encode = lambda x: [stoi[ch] for ch in x]\n",
    "decode = lambda x: \"\".join([itos[token] for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823a95fb-ed8b-4798-a5b0-f502c40ff7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tokens: [20, 46, 53, 53, 56, 2, 1, 25, 66, 1, 55, 42, 54, 46, 1, 50, 60, 1, 31, 50, 45, 45, 49, 42, 55, 61, 1, 31, 50, 55, 48, 49, 1, 23, 42, 59, 52, 50]\n",
      "Decoded Tokens: Hello! My name is Siddhant Singh Karki\n"
     ]
    }
   ],
   "source": [
    "# An Example of the tokenization\n",
    "example = \"Hello! My name is Siddhant Singh Karki\"\n",
    "tokens = encode(example)\n",
    "print(f\"Encoded Tokens: {tokens}\")\n",
    "print(f\"Decoded Tokens: {decode(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb8f1f-63c7-4ad0-bce8-e4e91a6ee4d2",
   "metadata": {},
   "source": [
    "### What we want is the  ```(n+1)th``` sequence is a label for ```nth``` sequece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795c4f71-cc37-4eab-ab5c-81ac889fbfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C --> H\n",
      "********************\n",
      "CH --> A\n",
      "********************\n",
      "CHA --> P\n",
      "********************\n",
      "CHAP --> T\n",
      "********************\n",
      "CHAPT --> E\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "samples = 5\n",
    "curr_list = [j for j in range(samples + 1)]\n",
    "for idx, idy in zip(curr_list, curr_list[1:]):\n",
    "    print(f\"{text[:idx+1]} --> {text[idy]}\")\n",
    "    print(\"*\" * 20)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683e9e8e-a3d2-4a8f-9ece-d005a7024188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa988705-f03a-40ba-89d4-e667eae1bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "split_size = 0.9\n",
    "n = int(split_size * len(data))\n",
    "train_set = data[:n]\n",
    "val_set = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c0673ecc-d1bf-434e-bf05-4a07f9df63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "block_size = 16\n",
    "vocab_size = len(chars)\n",
    "epochs = 1000\n",
    "log_iter = 100\n",
    "eval_ter = 200\n",
    "learning_rate = 1e-3\n",
    "n_embd = 64\n",
    "num_heads = 8\n",
    "n_blocks = 8\n",
    "dropout = 0.2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5e87370c-9dad-4542-a078-de39555653d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataloader from scratch\n",
    "def get_batches(split):\n",
    "    batch_data = train_set if split == \"train\" else val_set\n",
    "    idxs = torch.randint(len(data) - block_size - 1, (batch_size, ))\n",
    "    x = torch.stack([data[i:block_size+i] for i in idxs])\n",
    "    y = torch.stack([data[i+1:block_size+i+1] for i in idxs])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "49ad7a01-aaff-4bb4-adcc-334aaad3d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batches(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8692750a-926a-431b-ad9a-9d1357f4b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([8, 16])\n",
      "y shape: torch.Size([8, 16])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cec596f-3213-43af-bdaf-d5143baeacde",
   "metadata": {},
   "source": [
    "## Model Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c7a29-7abc-46d8-859d-4e52c80a6821",
   "metadata": {},
   "source": [
    "## Why do we need a Linear Projection in MultiHeadAttention?\n",
    "- Concatenation of the individual head outputs\n",
    "- Learning Richer Represntation: Even if the dim matches the embedding layer, The projection layer plays an important role in learning a combination of the outputs from different heads.\n",
    "- The proj. layer allows the model to lean how to optimally mix the information from each head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6afaff-a36d-465c-b239-c62787d46b80",
   "metadata": {},
   "source": [
    "## Why do we need a Feed Forward Layer?\n",
    "- Feature Transformation: The FF Layer processe each token independently and applied two linear transformations with an intermediate activation function.\n",
    "- Enhancing Non-Linearity: SA mechanism in a transformer is Linear, which means it can capture relationships between tokens but might not introduce enough non-linearity. FF layer adds a non-linear transformation through an activation function which allows the model to capture complex patterns and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b17b5-32a1-4b92-bde6-1e5258fcaf9d",
   "metadata": {},
   "source": [
    "## Why add LayerNorm?\n",
    "- Below is the training statistics without layer norm (Exploding gradients)\n",
    "- `step 0: train loss 4.7718, val loss 4.7709`\n",
    "-`step 100: train loss 2.5493, val loss 2.5492`\n",
    "-`step 200: train loss 60.1986, val loss 59.9926`\n",
    "-`step 300: train loss 558319.7500, val loss 557347.4375`\n",
    "\n",
    "- With Layer Norm:\n",
    "- `step 0: train loss 4.4536, val loss 4.4521\n",
    "step 100: train loss 2.6987, val loss 2.7117\n",
    "step 200: train loss 2.5321, val loss 2.5077\n",
    "step 300: train loss 2.4262, val loss 2.4186\n",
    "step 400: train loss 2.3479, val loss 2.3534\n",
    "step 500: train loss 2.2614, val loss 2.2863\n",
    "step 600: train loss 2.2484, val loss 2.2344\n",
    "step 700: train loss 2.2003, val loss 2.1847\n",
    "step 800: train loss 2.1532, val loss 2.1447\n",
    "step 900: train loss 2.1368, val loss 2.1196\n",
    "step 999: train loss 2.1124, val loss 2.0768`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "848be426-ed13-4569-86d9-039d5cd92018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # Calculating the affinities\n",
    "        wei = (q @ k.transpose(-2, -1)) * k.shape[-1]**0.5\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "# Till MultiHeadAttention we're still extracting feature (how are tokens reacting to each other\n",
    "# How much of \"Attention\" is one token emmitting to the other\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.multi_head = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' \n",
    "            We fork off and do some communication and come back.\n",
    "            Doing so adds gradients equally up to the original input during back propogation.\n",
    "            Addtionally distributes gradient equally to both the branches.\n",
    "            Gradients hop to every additional node.\n",
    "            There is a gradient superhighway .\n",
    "            In the beginning they contribute very less.\n",
    "            But during optimization, then the block over time kick in\n",
    "            Original paper does this a bit differently \n",
    "        '''\n",
    "        x = x + self.multi_head(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "        \n",
    "\n",
    "class GPT2Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, num_heads) for _ in range(n_blocks)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # pluck out the learned token embedding vectors for each token\n",
    "        token_embds = self.embedding_table(idx)\n",
    "        # pluck out the learned position embedding vectors for each token\n",
    "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
    "        x = token_embds + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return loss, logits \n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_cropped = idx[:, -block_size:]\n",
    "            loss, logits = self(idx_cropped)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # concat along the time dimension\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a98dab-5322-4509-9d32-8662b22457cc",
   "metadata": {},
   "source": [
    "# Model Code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "41a96a25-47f9-4b87-a70b-f069d6479c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2Transformer()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e218cf-71af-4d1e-bd4d-a4c3cf6edb64",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19583fb6-8a35-4faa-903d-32ca52d2aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.5045, val loss 4.5102\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batches(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/makemore/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/makemore/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[100], line 94\u001b[0m, in \u001b[0;36mGPT2Transformer.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     92\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     93\u001b[0m x \u001b[38;5;241m=\u001b[39m token_embds \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[0;32m---> 94\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x)\n\u001b[1;32m     95\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m     97\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/makemore/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/makemore/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/makemore/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/makemore/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/makemore/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[100], line 70\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' \u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m        We fork off and do some communication and come back.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m        Doing so adds gradients equally up to the original input during back propogation.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m        Original paper does this a bit differently \u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x))\n\u001b[1;32m     71\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/makemore/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/makemore/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[100], line 32\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 32\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h(x) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out)\n\u001b[1;32m     34\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/makemore/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/makemore/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_ter)\n",
    "        for k in range(eval_ter):\n",
    "            X, Y = get_batches(split)\n",
    "            loss, logits = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "for iter in range(epochs):\n",
    "    if iter % log_iter == 0 or iter == epochs - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses[\"train\"]:0.4f}, val loss {losses['val']:.4f}\")\n",
    "    xb, yb = get_batches(\"train\")\n",
    "    loss, logits = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e24c396b-c4eb-4655-992d-579b832fbd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "output = model.generate(context, max_tokens=10000)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3eca0caa-7b68-4889-97b5-215d545b68c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "they that knat, (as and whep dead still contis\n",
      "dowea. “Ick chough: add sand she knars look of nold book witt less corm a fanded as al a\n",
      "tain’t a knory hard the went,” said the poed, about all said in tand they _long ppeald as she put gooden and gating the said think! Would belf,\n",
      "fleeps dine, are all her taking for dra a geft one the chance. “Non yes, and thing sheas a know chagn dend butle bit Alice_, whif after as? sholk I wist nrad that por till: what ste\n",
      "ot_treton: one gamet dinked,” said the DTurble.\n",
      " “—You _. I’ve\n",
      "conted, said put a mong all the hat be was, go and all a\n",
      "going think, and,” said Alice wat thim it o take I at then, hizs\n",
      "alh, fus in as to mind: and no a they fats: all I dill_, with undennat hre went for fut a lost all all at lookmy\n",
      "now ,” Alice Tharpiar cumpling sol\n",
      "of,” the clakily\n",
      "to was mut poop on.”\n",
      "\n",
      "would wis to the hea? Alif all,” The Furt was\n",
      "staid Alich, and she ind—for they le. “It wo” dack cound an to cone, and\n",
      "ree as beg, on.\n",
      "\n",
      "“I fist litttge corthpat coking went the eyppayppy, and farmited—” scremed Alice themser not, beater it be handed\n",
      "for Ed “_s you shought do agar?” said “Th_ knot_ you said to imper throwe las\n",
      "wast— at “Pe that go,” and but her had at courmboat to othe it hi: of as it Alice Dore _yzooke, ad then arm she agat loting on a duckent at think\n",
      "undgling was: and the Came ind feelten the poot weil spen bark anl wimble, endersalf uscg os, annd you cheas a litt!” that’s was a droden to colg; “a li.\n",
      "\n",
      "“ShS hey sant a came of the oft come.”\n",
      "\n",
      "Al; “Wo shas: innf with fut said they _taile,” the Rabbout fard the her dimen, and it agk Turt_ Fitters\n",
      "the nam her now again, Alice they know\n",
      "lift the was,” her _‘Igkean,\n",
      "and fattly int ficked the wall firus in a not wa wioou roudd said.\n",
      "\n",
      "“I velais will yard wor,” and at they MaOce, as did\n",
      "hed then said tE all were had pawoing, at the that the lenged ind the of simen as all, all\n",
      "the cotryting and tuppening.\n",
      "“It said it theisunich, andver_ if henping chedn. “WhOH be way, but ind, and her\n",
      "went lask three of the toone,” noase addepen reatt _woud\n",
      "a for\n",
      "they Pised a large, withen, it that ed-the Dive tale you mead be _ny.”\n",
      "\n",
      "“I doon,’t tilkend on as ip far\n",
      " feellar tong riest of trie:\n",
      "and the talk.\n",
      "\n",
      "The Cas for no a foll he comen, “nd a upasayqueten ting wai,”n wne thim durned a much. “_a again in at you\n",
      "pick!” the Queen the Hatter.\n",
      "\n",
      "“Yer,” said Alice had peeptaststeny thrinking goind sheent, ey,\n",
      "and at sore ino Alit_ a down up a\n",
      "not fol then, will said at doe for _when aslN time\n",
      "all farted\n",
      "seeppan agar at littles wut whit “DObough wh thi I hard on wen it on me as to ourn. “I a the\n",
      "shit! Alick I_ litt,” said agk\n",
      "“It’f if with,ten and of\n",
      "scedent have a ind a dice dinnily at you it felt a’ll telalk head _lead. “I the hell Bend, sudded hought the to silen: qu\n",
      "stance fildder “bod had very thinking, rown we him with frole mor to sead reve a Rabbe little!” said the Harlaar someeting for thinkeng: age_ a I Ra“I whith she carking taken\n",
      "chiom se. \n",
      "“Walk his no poind arm,” said the Marcth: Alice the it spon plied a\n",
      "the stalling at in wit,” her said Af ving about ther agl againd—thinkeng at ind a rhin ack a Dormear no a it think of and Lacker oder ber nly, and whis on voou nolsed in said,” said Alice seetqay ping to a wNerencan’t thotersed bears Alice smemble. For hear dided about a gistfor grot ind the ritting,” the hrachon would had\n",
      "that peack—” said Aliceon her loDunctal (Fit_ jury noath cough,” ether ted theight think\n",
      "a reppian.\n",
      "\n",
      "“I sea!”\n",
      "\n",
      "\n",
      "“I’ did\n",
      "all at that\n",
      "lest,” futted to poop they of that fat all goinece, the put asent happ.\n",
      "\n",
      "\n",
      "“I glided don loat\n",
      "fith a be as all you.\n",
      "\n",
      "Vgain aw and a\n",
      "ls ancon, “Alice thu sop titee up _witixied,” her Hatt shis, “I do and you isk and,” she the jur!” stided then by a milase, unded aft!” kever the a _mBew witomeng\n",
      "lound, her at goll in a lacking afl, and publend by and the My swoRut as, Pand wis a do nover a\n",
      "tone with gen by noit her the will af up and foort than, Alice; “But on\n",
      "fq, a said she\n",
      "was\n",
      "what was a\n",
      "nalst; Mout And,” said Alice _porected was,” said Alice; for thindos, very the quistlence catten as a by somess me make again; and are they ous li\n",
      "meppan. dea aud ttink the for I caand:! I   the able’ve went\n",
      "littall it king thrat,” said Alice was coulre comfuld agx; at readen the nore doesf, shead!” thought the Majces,,” said the thout\n",
      "and well a\n",
      "\n",
      "“I whith\n",
      "_Ward round thing sone, they with the ded mad or of the\n",
      "out her all he Ran! Ataice feea—Off!X,” the Dometten dided back as would smenge in was\n",
      "sheated Cat meat esng!” again,\n",
      "Alice hand reversall a likeng on it, and with flow theimy_ thu litt,” the her ifted spo in spa kinp_ a woat, “but al the hat Alis’s then gos her tse Hared, and’s bee fole wench I shan’t you the was\n",
      "not had if a mow it again quihe; the dastat it as the Hott readeds all \n",
      "shing the Maj, out they was stysing chemully tome at a herm*uddee bit fart ind: forh I _poin not\n",
      "the all by again.\n",
      "She Queen in, quee ased it,” and but lasten\n",
      "arcont vire. Se see, will and drectand at one amat Alice, It siped\n",
      "think adreneres it\n",
      "no and juinde swo\n",
      "the Marl I of the pon aseail, for\n",
      "indems, and she do. (“Wouth you shut belf a that she for af the woutt a go?” the Dormalliing witur the _on wondenting sounbe?\n",
      "“Loot,” and Alice man the talkeal, “swink about quers, Alice was no arge ive! I, “Twa they a wainq a aff as undenf-not\n",
      "slappy itlasn it shancup_, is such a und be hid poind a go the mooke:\n",
      "and\n",
      "annx, “I they mustant wa teng to, but Alit-Alice happrowiard thom! thinM think\n",
      "you meam of they mathe\n",
      "bitti, and dow the again son.” Alice they you musly af rooming thought the heren\n",
      "and swo. But at were not\n",
      "ch,” not the Duch moning are ifue theT a Rif the_ uS speap the Mock Thrie wardere a all, and for the\n",
      "re sold with _that the Alice. “UYy us seembeas id a the tand think again’ gead down for cortwoutnch,” cain the Mow—, and up a ds abbout looked\n",
      "at fill you’Q at a was, and it again, out put it stuck that good, do—” _cmakuing this as wall with that eyan,” said the\n",
      "the quite itfelved,” the Mas bout, but’s little: “—N_!” (“I deab yok u_ to her\n",
      "Mes the the talked wast then you\n",
      "shink the Mouse so,” the said Alice she was it meent wond be over con wit\n",
      "ted chemed in to meenton’t: and waalken all ind a reave not and a Alit thinded,\n",
      "to and after it to fooul_ you that Whelered all _we spreall, time as in the Alice\n",
      "lence, and smick—wutt I cwo:—OV’t court she waill Alice, Alice gaid. “Alice gonett in gurd to voormings ad sompoy ar of the leave as\n",
      "in teell abbou fowing ter Rok said a wind rene\n",
      "a besh becuck at ueded hish aI speod all,” poodence a” Sho. “You jere a steemed, ind whis, they jury as, I gemppoll a mam goest it sfo breclanf, frriutileng hand his\n",
      "treat thaus: think that it!” sy said “ind a Berent the Mouse they hing. “I’d form yes ay ren,\n",
      "and went! I whink she kead, trean them, and not you to the Dod\n",
      "tempping with en widd Noow. The Cat) ffister_ toth at a not wat ill she litte,” cand\n",
      "Alice, wen done: “s woust it’s you wut Alicce.\n",
      "“I doin then’d to to of the fore; and as le littesperean as did that that lest. “I’t had the\n",
      "the\n",
      "gepcot feet it a with I a veres the sucknd.” At daid it _you shoff ever just?:\n",
      "“Lee with Kech all will\n",
      "the_ Mox a\n",
      "duct at loudd walken_ the garnag.\n",
      "“I cand lif they they bock it a caure’s have wer think bell a the was piorps his the chat dock. “Fund!” that on _had: noitwainD when, whith it a that queesty, “and coldd a thiok; DHN?” And the think “_TwEni) and Bull\n",
      "ly she I somean the RAlice stilly,” eperea thirt, and  noo for.\n",
      "\n",
      "\n",
      "“and fut lutt oun a thinng Bilk litttle, “Ju riaded, and it that shiff,”, to Ali— at a site\n",
      "Alicampeat about. Alice, I had a\n",
      "nol, are wam it\n",
      "think ulle lofrook the dake_ Alice pat a what it a voice, all\n",
      "pzing\n",
      "about the wasbe had eppelly\n",
      "cate wink the verthing and at about befht her war' looke.\n",
      "\n",
      "“CY, ond S a will beeht und loke the las. A wond querean’te as got that knist loy.\n",
      "\n",
      "“‘__thy?” Cas at Al I not litttengs, and ind, again over an of, the with I’ve ar gairerfoing a dill wered aRd dish talking as acan a lest bojew back if was seeppry\n",
      "that inf what no haest woy,” \n",
      "“I mad hom coortean\n",
      "    Bile stinke.\n",
      "\n",
      "“YeVding was a thim_ the’s rabeen. Anic all\n",
      "toast aind of sf think the staid it herse all wrosend a now it—” the MoH as’” she houdden again. “I’d but Sookt she theirpils,_ I with a minat stakes at lont od the chring\n",
      "ind\n",
      "the it sudnce arave offfd at you afl you and goning on both on,” “be shemed, and Alie sis no was no the seen and,” the Dormousawn Edy ’Aly ind said they did\n",
      "qy ab a wawn ever caod ol long on to to said in the that triake\n",
      "and of ner a whout mare to age a wait,” said t\n",
      "“On, you on coveres arm it,. AfOcou’d  said, for her: and go:_ athed it was take cobre grill it hall cong kings, “_ very wayol ind os rus of:\n",
      "\n",
      "“Wit’s at the Queen tille\n",
      "to czing ave a ch! Whithy: on go indes, af! pick!”\n",
      "\n",
      "“It’s whallu a dy in a don_’” comman and that)\n",
      "they not’s muth drite\n",
      "wa—PTter ly at the do swaick\n",
      "a herster,” said thinirnes, and thinke. The Ditthouse, and\n",
      "fror on thind: camh a now most a gon, and deet a could a beinh,\n",
      "I cearce,\n",
      "and quittes: and live a mep your sald a long all think to ffftew to a mart as way house che salk Foot foud Alipper leadd about on thing, and ind a\n",
      "loead about in a shen,\n",
      "ind very back, and sused supt sied to Hatt gob’t ner at of: Alice,” ned “beffind her thate ver henw her huse lasty, “Alice ageend but not ad minh wand will tide_ the Gy must somed and,” sai the Kind i they. whenm om ful\n",
      "ping theremange-but up whas o— tate_ the Marce crisered a wop hing a\n",
      "thim_ _gan at_ up as get a-lakin.\n",
      "\n",
      "“I soppe! Fit’s they,” said Alice suck\n",
      "bitte ctoremansping At a daight had she cough fastim: “a\n",
      "my,” said thind the\n",
      "Alice Dou be wny acaused,\n",
      "“I for you,” said threlf the wong come take, and\n",
      "of cloveried, the was, and rowint ill the behen a spake cuamp at Alith, your as epl, all, and ruel parater wor the timuneck cormould:\n",
      "\n",
      "“I wa_ Yomem eab, wome quite shouse fadderfed talked good in ho to the could be after hear pitole that “a gett,” said\n",
      "Alice. “Pild and a\n",
      "sficed agai\n"
     ]
    }
   ],
   "source": [
    "print(decode(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe473439-58e0-4a72-8eb5-86cd5928ea2f",
   "metadata": {},
   "source": [
    "# Masked Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "19a84d8b-0238-4287-8a4c-2adad075d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn((batch_size, block_size, n_embd))\n",
    "y = torch.randn((batch_size, block_size, n_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "aa5f8934-fcce-438c-afd1-367e825c8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = nn.Linear(n_embd, head_size, bias=False)\n",
    "query = nn.Linear(n_embd, head_size, bias=False)\n",
    "value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "k = key(X)\n",
    "q = query(X)\n",
    "v = value(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "904afba7-09ae-4458-8b88-b4b156efec72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "3801a477-1f43-473e-878a-f450d3857cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B, T, C) @ (B, C, T)\n",
    "wei = (q @ k.transpose(-2, -1)) * k.shape[0]**-0.5\n",
    "mask = torch.tril(torch.ones(batch_size, block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5744a7a4-bd54-4a58-af1a-b80deaff0b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1653, grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "e01538fc-b38a-48dc-b0e4-1f3e4d569291",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = wei.masked_fill(mask == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "edc16bbe-31ca-40c0-97d1-0cbbd4ad7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = F.softmax(wei, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "552ec49e-e652-489e-98c9-094150723297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6074, 0.3926, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2696, 0.3481, 0.3823, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1844, 0.2523, 0.2507, 0.3126, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1398, 0.2291, 0.2481, 0.2378, 0.1453, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1095, 0.1839, 0.1945, 0.1197, 0.2366, 0.1557, 0.0000, 0.0000],\n",
       "        [0.1638, 0.1249, 0.0948, 0.1708, 0.1656, 0.1554, 0.1248, 0.0000],\n",
       "        [0.1059, 0.1645, 0.1684, 0.1861, 0.0689, 0.0699, 0.1075, 0.1289]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "dd97d9fb-526b-4eac-8780-be3029acbde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4090, -0.0844,  0.4595,  0.0465,  0.5468,  1.1730, -0.3282, -0.3627,\n",
       "          0.1783,  0.0353, -0.2119, -0.3698,  0.4134,  0.2859,  0.2247, -0.6131],\n",
       "        [-0.7410,  0.0779, -0.2925,  0.2071, -0.5912,  0.3608, -0.0498,  0.7420,\n",
       "         -0.2828, -0.3913, -0.2852, -0.9858, -0.2219, -0.2099, -0.1447,  0.2150],\n",
       "        [ 0.3926,  0.2877,  0.2059, -0.3759,  0.1268, -0.2432, -0.4747,  0.4679,\n",
       "          0.6460,  0.7017, -0.6274,  0.3068,  0.0086, -0.1442, -0.3745,  0.2156],\n",
       "        [-0.6158,  0.9428,  0.1268,  0.2106,  0.1436,  0.0402,  0.0900, -0.2539,\n",
       "          0.5076,  0.9316,  1.0298, -0.8456, -0.8128,  0.4148, -1.2114, -0.4425],\n",
       "        [ 0.6647, -0.5455,  0.5326, -0.9780, -0.2174,  0.9473, -0.1027,  0.8601,\n",
       "          1.3140,  0.1001,  0.1058,  0.4890,  0.2137, -0.1716,  0.4304,  0.1098],\n",
       "        [-1.0986, -0.0348,  0.3473, -0.4341,  0.2286,  1.0323, -0.3086,  0.1645,\n",
       "          0.2484,  0.4556, -0.4360,  0.8109, -0.4500, -0.1652, -0.0652,  0.6274],\n",
       "        [-0.0714,  0.1240, -1.1368, -0.5158,  0.9738, -0.6874, -0.2227,  0.2696,\n",
       "         -0.1966, -0.7407,  0.0658,  0.6555,  0.3453, -1.2774,  1.2191,  0.7358],\n",
       "        [-0.2691,  0.0156,  0.4865, -0.0356,  0.2738,  0.4640, -0.6319, -0.8009,\n",
       "          0.2255,  0.4807, -0.2532, -0.0299,  0.5815, -0.7905, -0.1502, -0.1433]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa6a1a-d35a-4f32-992b-c973cd4c4295",
   "metadata": {},
   "source": [
    "# Each one of these attention values communicate only backward, not in the future (decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5b708236-db7a-41e1-a21f-adc3c06bad7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4090, -0.0844,  0.4595,  0.0465,  0.5468,  1.1730, -0.3282, -0.3627,\n",
       "          0.1783,  0.0353, -0.2119, -0.3698,  0.4134,  0.2859,  0.2247, -0.6131],\n",
       "        [-0.5393, -0.0206,  0.1643,  0.1095,  0.1000,  0.8542, -0.2189,  0.0710,\n",
       "         -0.0027, -0.1321, -0.2407, -0.6116,  0.1640,  0.0913,  0.0797, -0.2881],\n",
       "        [-0.2181,  0.1144,  0.1008, -0.0591, -0.0099,  0.3489, -0.2873,  0.3394,\n",
       "          0.1966,  0.1416, -0.3963, -0.3255,  0.0375, -0.0511, -0.1329, -0.0081],\n",
       "        [-0.3565,  0.3709,  0.1022,  0.0324,  0.0284,  0.2589, -0.1639,  0.1582,\n",
       "          0.2822,  0.3749,  0.0536, -0.5043, -0.2317,  0.0933, -0.4676, -0.1431],\n",
       "        [-0.1794,  0.2224,  0.1558, -0.1313, -0.0250,  0.3334, -0.1686,  0.3000,\n",
       "          0.4320,  0.3254,  0.0096, -0.3314, -0.1531,  0.0298, -0.3202, -0.0722],\n",
       "        [-0.1921,  0.0394,  0.2319, -0.3038, -0.0228,  0.5372, -0.1990,  0.3865,\n",
       "          0.5036,  0.2746, -0.1173, -0.0213, -0.1107, -0.0520, -0.1282,  0.0850],\n",
       "        [-0.2971,  0.1040,  0.0802, -0.2599,  0.1733,  0.4525, -0.1823,  0.2359,\n",
       "          0.3734,  0.1775,  0.0041, -0.0103, -0.0895, -0.1357, -0.0104,  0.0788],\n",
       "        [-0.2870,  0.2031,  0.0603, -0.1428,  0.1497,  0.2734, -0.2401,  0.1117,\n",
       "          0.2914,  0.2520, -0.0321, -0.1501, -0.0472, -0.2139, -0.1518,  0.0364]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]@v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198bc3a-0a36-4c62-b45b-93d9ddeadce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
